{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pPCA_experiments.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Decision-Making-VAE Experiements "
      ],
      "metadata": {
        "id": "UsLzk_LDdtN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "\n",
        "import torch; torch.manual_seed(0)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils\n",
        "import torch.distributions\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "qEAqiVGjDsOe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First experiment : pPCA\n",
        "In the following code, latent_dim  will relate to the dimension of the latent space"
      ],
      "metadata": {
        "id": "U4M2MbwgDhol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o3mORYj6DfxR"
      },
      "outputs": [],
      "source": [
        "#Generating data for pPCA\n",
        "\n",
        "seed = 0\n",
        "dim_z=10 # d\n",
        "dim_x=100 # p\n",
        "nu=0.5\n",
        "n_samples=1000\n",
        "np.random.seed(seed)\n",
        "# Parameters\n",
        "\n",
        "A = 1 / np.sqrt(dim_z) * np.random.normal(size=(dim_x, dim_z))\n",
        "px_mean = np.zeros((dim_x,))\n",
        "\n",
        "# conditional covariance\n",
        "\n",
        "#Sigma_x_cond_z = nu * np.diag(np.random.normal(loc=1, scale=2, size=dim_x) ** 2)\n",
        "gamma = nu * np.diag(np.random.normal(loc=1, scale=2, size=dim_x) ** 2)\n",
        "#inv_Sigma_x_cond_z = np.linalg.inv(Sigma_x_cond_z)\n",
        "inv_gamma = np.linalg.inv(gamma)\n",
        "px_var = gamma + np.dot(A, A.T)\n",
        "#posterior\n",
        "\n",
        "inv_pz_condx_var = np.eye(dim_z) + np.dot(np.dot(A.T, inv_gamma),A)\n",
        "pz_condx_var = np.linalg.inv(inv_pz_condx_var)\n",
        "mz_cond_x_mean = np.dot(pz_condx_var, np.dot(A.T, inv_gamma))\n",
        "\n",
        "\n",
        "covar_joint = np.block([[np.eye(dim_z), A.T], [A, px_var]])\n",
        "pxz_log_det = np.log(np.linalg.det(covar_joint))\n",
        "pxz_inv_sqrt = sqrtm(np.linalg.inv(covar_joint))\n",
        "\n",
        "#generating data\n",
        "data = np.random.multivariate_normal(px_mean, px_var, size=(n_samples,))\n",
        "\n",
        "\n",
        "#generating data\n",
        "#x = np.random.multivariate_normal(np.zeros((dim_x,)),Sigma_x_cond_z+np.dot(A,A.T),size=(n_samples,))\n",
        "#z = np.random.multivariate_normal(np.zeros((dim_z)),np.eye((dim_z)),size=(n_samples,))\n",
        "\n",
        "\n",
        "# posterior expression\n",
        "#inv_Sigma_z_cond_x = np.eye(dim_z) + np.dot(np.dot(A.T, inv_Sigma_x_cond_z), A)\n",
        "#print(inv_Sigma_z_cond_x.shape)\n",
        "#Sigma_z_cond_x = np.linalg.inv(inv_Sigma_z_cond_x)\n",
        "\n",
        "#Mz_cond_x = np.dot(Sigma_z_cond_x, np.dot(A.T, inv_Sigma_x_cond_z))\n",
        "#z_cond_x = np.random.multivariate_normal(np.dot(Mz_cond_x,x),Sigma_z_cond_x,size=(n_samples,))\n",
        "\n",
        "# other stuff we need \n",
        "#covar_joint = np.block([[np.eye(dim_z), A.T], [A, Sigma_x_cond_z+np.dot(A,A.T)]])\n",
        "#pxz_log_det = np.log(np.linalg.det(covar_joint))\n",
        "#pxz_inv_sqrt = sqrtm(np.linalg.inv(covar_joint))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variational AutoEncoder structure\n",
        "\n",
        "latent_dims = dim_z\n",
        "\n",
        "class VariationalEncoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(VariationalEncoder, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(dim_x, dim_x//2)\n",
        "        self.linear2 = nn.Linear(dim_x//2, latent_dims)\n",
        "        self.linear3 = nn.Linear(dim_x//2, latent_dims)\n",
        "        self.mu =0\n",
        "        self.var = 0\n",
        "\n",
        "        self.N = torch.distributions.Normal(0, 1)\n",
        "        self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
        "        self.N.scale = self.N.scale.cuda()\n",
        "        self.kl = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        mu =  self.linear2(x)\n",
        "        sigma = torch.exp(self.linear3(x))\n",
        "        z = mu + sigma*self.N.sample(mu.shape)\n",
        "        # N(0, I)  -> p(z)\n",
        "        # KL(N(mu,sigma);N(0,I)) = sum(sigma²+mu²-log sigma - 1/2)\n",
        "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum() # KL(q,p(z))\n",
        "        self.mu = mu\n",
        "        self.var =sigma\n",
        "        return z\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.linear1 = nn.Linear(latent_dims, dim_x//2)\n",
        "        self.linear2 = nn.Linear(dim_x//2, dim_x)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = F.relu(self.linear1(z))\n",
        "        z = torch.sigmoid(self.linear2(z))\n",
        "        return z.reshape((-1, 1, dim_x)) \n",
        "           \n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "\n",
        "        self.encoder = VariationalEncoder(latent_dims)\n",
        "        self.decoder = Decoder(latent_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Encodes the input by passing through the encoder network and returns the latent codes.\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def get_distribution(self):\n",
        "      return(self.encoder.mu,self.encoder.var)"
      ],
      "metadata": {
        "id": "ko6rWKjRDu4j"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(autoencoder, data, epochs=20):\n",
        "    opt = torch.optim.Adam(autoencoder.parameters())\n",
        "    for epoch in range(epochs):\n",
        "        for x in data:\n",
        "            x = x.float().to(device) # GPU\n",
        "            opt.zero_grad()\n",
        "            x_hat = autoencoder(x)\n",
        "            loss = ((x - x_hat)**2).sum() + autoencoder.encoder.kl\n",
        "            #(sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum() + ((x - x_hat)**2).sum()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "Gz6JDX_sD13c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = torch.utils.data.DataLoader(data,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True)"
      ],
      "metadata": {
        "id": "Wmf-qbUFlGFF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VariationalAutoencoder(latent_dims).to(device) # GPU\n",
        "vae = train(vae, data_loader)"
      ],
      "metadata": {
        "id": "CuSCkgA0D3to"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_mu,q_sigma = vae.get_distribution()"
      ],
      "metadata": {
        "id": "w-ZdU3CeD46i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(q_mu.cpu().detach().numpy(),q_sigma.cpu().detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBNtzXu7D6hE",
        "outputId": "5a28e69b-964c-4b6a-aeba-b6e2cb41bb44"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.88169205  0.8570416   0.63243055 -0.3322851   1.4059932   1.0859292\n",
            "   0.33169502  0.64419377 -1.6628016  -0.33515942]] [[0.5551765  0.25311744 0.35380572 0.2844929  0.22901307 0.2849016\n",
            "  0.3074084  0.27310145 0.27291787 0.1789326 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plugin estimator for the paper ELBO toy example"
      ],
      "metadata": {
        "id": "YtTnGHTGD8dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "\n",
        "q_mu_a = q_mu.cpu().detach().numpy()[0]\n",
        "q_var_a = q_sigma.cpu().detach().numpy()[0]\n",
        "f = lambda z: z[0] > threshold \n",
        "\n",
        "def Q_plug(n,f):\n",
        "  z_array = np.random.multivariate_normal(q_mu_a,np.diag(q_var_a),size=(n,))\n",
        "  print(z_array.shape)\n",
        "  return(np.sum(f(z_array)/n))\n",
        "\n",
        "Q_plug(1000,f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLf2y4h3EAqo",
        "outputId": "e1756220-c859-442e-94e9-e8e5be7ae710"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.005"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some tries on the SNIS Estimator for pPCA"
      ],
      "metadata": {
        "id": "f6Gjo6n2ED7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IWVariationalEncoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(IWVariationalEncoder, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(dim_x, dim_x//2)\n",
        "        self.linear2 = nn.Linear(dim_x//2, latent_dims)\n",
        "        self.linear3 = nn.Linear(dim_x//2, latent_dims)\n",
        "        self.mu =0\n",
        "        self.std = 0\n",
        "\n",
        "        self.N = torch.distributions.Normal(0, 1)\n",
        "        self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
        "        self.N.scale = self.N.scale.cuda()\n",
        "        self.kl = 0\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        mu =  self.linear2(x)\n",
        "        sigma = torch.sigmoid(self.linear3(x))\n",
        "        z = mu + sigma*self.N.sample(mu.shape)\n",
        "        # N(0, I)  -> p(z)\n",
        "        # KL(N(mu,sigma);N(0,I)) = sum(sigma²+mu²-log sigma - 1/2)\n",
        "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum() # KL(q,p(z))\n",
        "        self.mu = mu\n",
        "        self.std =sigma\n",
        "        return z\n",
        "\n",
        "\n",
        "class IWVariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(IWVariationalAutoencoder, self).__init__()\n",
        "        \n",
        "        self.encoder = IWVariationalEncoder(latent_dims)\n",
        "        self.decoder = Decoder(latent_dims)\n",
        "\n",
        "    def loss(self,x, k):\n",
        "      \"\"\"\n",
        "        Computes the IWVAE loss functionn using the IWELBO\n",
        "      \"\"\"\n",
        "      z= self.encoder(x)\n",
        "      z_mu, z_std = self.encoder.mu, self.encoder.std\n",
        "      x_hat = self.decoder(z)\n",
        "      x_hat = torch.sigmoid(torch.flatten(x_hat, start_dim=1))\n",
        "\n",
        "      log_p_z = torch.distributions.Normal(0, 1).log_prob(z).sum(dim=-1)\n",
        "      log_p_xGz = torch.distributions.Bernoulli(x_hat).log_prob(x).sum(dim=-1)\n",
        "      log_q_zGx = torch.distributions.Normal(z_mu, z_std).log_prob(z).sum(dim=-1)\n",
        "\n",
        "      log_weights = log_p_z + log_p_xGz - log_q_zGx\n",
        "      elbo = log_weights.logsumexp(dim=0) - np.log(k)\n",
        "\n",
        "      return -elbo.mean()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ],
      "metadata": {
        "id": "-rUS0bb7EDSs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IWtrain(autoencoder, data, epochs=20):\n",
        "    opt = torch.optim.Adam(autoencoder.parameters())\n",
        "    for epoch in range(epochs):\n",
        "        for x in data:\n",
        "            x = x.float().to(device) # GPU\n",
        "            opt.zero_grad()\n",
        "            x_hat = autoencoder(x)\n",
        "            loss = autoencoder.loss(x,autoencoder.encoder.kl)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "n9HjEbtcELWo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = IWVariationalAutoencoder(latent_dims).to(device) # GPU\n",
        "vae = IWtrain(vae, data_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "rwCctj5OENAH",
        "outputId": "fcea8c1a-fec6-4225-e325-07fd7687929a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ff5622e0f5c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIWVariationalAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIWtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-48a5c2eeb80c>\u001b[0m in \u001b[0;36mIWtrain\u001b[0;34m(autoencoder, data, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a18cdfae743d>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, k)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mlog_p_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m       \u001b[0mlog_p_xGz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m       \u001b[0mlog_q_zGx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/bernoulli.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             raise ValueError(\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0;34m\"Expected value argument \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0;34mf\"to be within the support ({repr(support)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected value argument (Tensor of shape (1, 100)) to be within the support (Boolean()) of the distribution Bernoulli(probs: torch.Size([1, 100])), but found invalid values:\ntensor([[-3.2348e-01, -5.9775e-01, -7.9160e-01, -1.1065e+00,  2.1300e+00,\n         -6.6241e-01,  1.8396e+00,  3.6051e-01, -1.0425e+00, -1.2303e-01,\n          5.6439e-01, -8.6363e-01, -1.0600e+00,  5.8991e+00,  1.1894e+00,\n          2.0698e-01,  2.4650e+00, -1.1921e+00, -6.9102e-01, -2.0201e+00,\n         -1.1339e+00,  4.8740e-01, -6.1672e-01,  1.1519e-01,  3.2189e+00,\n          3.9071e-01, -1.3282e+00, -2.7175e+00,  7.2342e-03, -1.2038e+00,\n          2.1229e+00,  7.5311e-01, -1.7465e+00, -1.5678e+00,  1.0226e+00,\n          6.7644e-01,  3.6514e+00, -2.6420e+00, -6.9644e-01,  1.3964e-02,\n         -1.7628e+00,  1.7719e+00,  1.9017e+00, -1.0244e+00,  5.4997e+00,\n          3.0031e+00,  8.1986e-01, -2.7853e-01, -1.4395e+00,  1.0006e+00,\n          7.0407e-01, -1.1510e+00, -2.5698e+00, -1.5423e+00,  3.3006e+00,\n          1.1690e+00, -7.5707e-01, -1.5517e+00,  1.8605e+00,  5.7613e-02,\n         -7.8617e-01,  9.4896e-01, -9.0675e-01,  4.5524e-01, -2.4513e+00,\n          1.4704e+00, -1.7216e+00,  1.4651e+00, -2.8450e+00, -1.9902e+00,\n         -8.0705e-01, -3.5289e-02, -5.6897e-01, -9.4393e-01, -2.2500e+00,\n         -3.1559e-01,  3.3386e+00, -5.5022e-02, -1.3217e+00,  1.9880e+00,\n         -8.1323e-01, -4.6472e-01, -1.7228e-01,  8.2721e-01, -1.0468e-01,\n          4.3951e-01,  2.8800e-01,  1.3696e+00,  1.5984e+00,  2.2748e+00,\n         -4.5307e-01, -6.3944e-01,  1.5467e+00,  2.1392e-01,  7.6077e+00,\n          7.8215e-01, -1.0621e+00,  2.2098e+00,  2.9050e+00,  2.8441e-01]],\n       device='cuda:0')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "F9i0MpRWEPXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UX6ybXPaEO41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}